\documentclass[letterpaper]{article}
\usepackage[pass,showframe]{geometry}

\usepackage{ijcai17}
\usepackage{times}
\usepackage{graphicx}
\usepackage{cleveref}

\usepackage[ruled,vlined]{algorithm2e}
\usepackage{MnSymbol,wasysym}
\usepackage{latexsym}

\usepackage{tikz}

\usetikzlibrary{graphs}

\newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}
\newcommand{\citep}[1]{\cite{#1}}

% TODO: use operatorname for these
\DeclareMathOperator{\V}{V}
\DeclareMathOperator{\E}{E}

\newcommand{\BigO}[1]{\ensuremath{\operatorname{O}\left(#1\right)}}

\newcommand{\examplexG}[5] {
    \begin{minipage}{.2\textwidth}
    \tikz {
        \graph [nodes={draw, circle, minimum width=.6cm}, circular placement, radius=1cm,
                clockwise=5] {
                    1[label=90:#1],2[label=0:#2],3[label=0:#3],4[label=180:#4],5[label=180:#5];
            1--4; 1--5; 2--3; 2--5; 3--5;
        };
    }
    \end{minipage}
}
\newcommand{\examplexH}[6] {
    \begin{minipage}{.2\textwidth}
    \tikz {
        \graph [nodes={draw, circle, minimum width=.6cm}, circular placement, radius=1.1cm,
                clockwise=6, phase=60] {
                    a[label=0:#1],b[label=0:#2],c[label=0:#3],d[label=180:#4],e[label=180:#5],f[label=180:#6];
            a--b; a--c; a--e; b--d; b--f; c--d; c--e; c--f; d--f; e--f;
        };
    }
    \end{minipage}
}

\title{A Partitioning Algorithm for Maximum Common (Connected) Subgraphs\thanks{This work
was supported by the Engineering and Physical Sciences Research Council [grant
numbers EP/K503058/1 and EP/M508056/1]}}
\author{Ciaran McCreesh \and Patrick Prosser \and James Trimble \\
University of Glasgow, Glasgow, Scotland \\
j.trimble.1@research.gla.ac.uk}

\begin{document}

\maketitle

\begin{abstract}
    We came up with an important new algorithm for the maximum common (connected)
    subgraph problem. This is an important problem, with at least two important
    applications.
\end{abstract}

\section{Introduction}

\citet{UpcomingAAAIPaper}

\citet{DBLP:conf/cp/McCreeshNPS16}

\citet{DBLP:conf/cp/NdiayeS11}

Given graphs $G=(\V(G), \E(G))$ and $H=(\V(H), \E(H))$, the \emph{Maximum Common
Subgraph (MCS)} problem is to find a graph with as many vertices as possible
that is isomorphic to induced subgraphs of both $G$ and $H$. Equivalently, MCS
is the problem of finding a maximum-cardinality set of pairs $M = \{(v_1, w_1),
\dots, (v_{|M|}, w_{|M|})\}$, with $v_i \in V(G)$ and $v_i \in V(H)$ for all $i$,
such that for each pair $i,j$ ($1 \leq i < j
\leq |M|$), $v_i$ and $v_j$ are adjacent in $G$ if and only if $w_i$ and $w_j$
are adjacent in $H$.

The most successful existing approaches to optimally solving MCS are
reformulation to the maximum clique problem and constraint programming
\cite{DBLP:conf/cp/McCreeshNPS16}. In this paper, we introduce a new branch and bound
algorithm for MCS which does not require a domain to be explicity maintained in
memory for each vertex in $\V(G)$. The algorithm maintains labellings of the
unmatched vertices in $\V(G)$ and $\V(H)$ such that a pair $(v, w)$ with $v \in \V(G)$
and $w \in \V(H)$ can be added to the matching if and only if $v$ and $w$ have the
same label.

\section{Preliminaries}

Let $N(v, G)$ denote the neighbourhood of a vertex $v$ in a graph $G$.  We
extend this notation to denote the set of vertices that are adjacent to all
vertices in a set $U$: $N(U, G) = \bigcap_{v \in U} N_G(v)$.

\section{Walkthrough}

Suppose we are given the graphs $G$ and $H$ in Figure~\ref{fig:alg1}.  Graphs $G$ and $H$
have no common subgraph with five vertices. However, they do have common
subgraphs with four vertices; the mapping $\{1a, 2f, 3d, 5b\}$ is one example.

\begin{figure}[ht]
\centering
    \examplexG{}{}{}{}{}
    \examplexH{}{}{}{}{}{}
\caption{Graphs $G$ and $H$}
\label{fig:alg1}
\end{figure}

The algorithm proceeds by building up a mapping $M$, starting with the empty
mapping $\{\}$. Begin by (arbitrarily) mapping vertex $1$ in $G$ to vertex $a$
in $H$, giving $M=\{1a\}$.  Label each unmatched vertex according to whether it
is adjacent to $1$ (in $G$) or $a$ (in $H$), as shown in Figure~\ref{fig:alg2}.
Adjacent vertices have label 1; non-adjacent vertices have label 0.  It is
straightforward to see that we can extend $M$ with a vertex mapping $vw$, with
$v \in \V(G)$ and $w \in \V(H)$, if and only if $v$ and $w$ have the same label.  This
property is the algorithm's key invariant.

\begin{figure}[ht]
\centering
    \examplexG{$(1a)$}{0}{0}{1}{1}
    \examplexH{$(1a)$}{1}{1}{0}{1}{0}
\caption{Step 2}
\label{fig:alg2}
\end{figure}

Next, extend the mapping by pairing a vertex in $G$ with a vertex in $H$ of the
same label; we will choose to map vertex $2$ to vertex $d$, giving $M=\{1a,
2d\}$ (Figure~\ref{fig:alg3}).  Each unmatched vertex $v \in \V(G)$ is labelled
with a two-character bit string, indicating whether $v$ is adjacent to each of
the two matched vertices in $\V(G)$ (vertices $1$ and $2$).  For example, vertex
$3$ is labelled $01$, indicating that it is not adjacent to $1$ but is adjacent
to $2$.  Labels are given to vertices in $\V(H)$ in a similar fashion, showing
whether each vertex is adjacent to $a$ and $d$.

Observe that our invariant is maintained: we can extend $M$ by a vertex pairing if
and only if the two vertices have the same colour.

\begin{figure}[ht]
\centering
    \examplexG{$(1a)$}{$(2d)$}{01}{10}{11}
    \examplexH{$(1a)$}{11}{11}{$(2d)$}{10}{01}
\caption{Step 3}
\label{fig:alg3}
\end{figure}

The algorithm proceeds by backtracking search. When we have explored the full
search space of matchings containing $\{1a, 2d\}$, we try reassigning $2$ to
$f$.  Since $d$ and $f$ are the only vertices to which $2$ can be matched given
the decision to match $1$ to $a$, we lastly explore the possibility that $2$ is
left unmatched, by giving $2$ the label $\bot$ and selecting another vertex in
$\V(G)$ to assign.



\section{The Lilybank Algorithm}

\begin{algorithm}
\DontPrintSemicolon
\nl $expand(\mathit{future},matching)$ \;
\nl \Begin{
\nl \lIf {$future = \emptyset$ \bf{and} $|matching|  > |incumbent|$}{$save(matching)$}
\nl \lIf {$future = \emptyset$}{return}
\nl $bestFuture \gets 0$ \;
\nl \lFor {$\langle P,T \rangle \in future$}{$bestFuture \gets bestFuture + min(|P|,|T|)$}
\nl \lIf {$|matching|  + bestFuture \leq |incumbent|$}{return}
\bigskip
\nl $\langle P,T \rangle \gets select(future)$ \;
\nl $v \gets select(P)$ \;
\nl \For {$w \in T$}{
\nl    $matching \gets matching + (v,w)$\;
\nl    \bf{Let} $future^{'} \gets \emptyset$ \;
\nl    \For {$\langle P^{'},T'^{'}\rangle \in future$}{
\nl        \bf{Let} $P^{''} \gets P^{'} \cap N(v,G) \setminus \{v\}$ \;
\nl        \bf{Let} $T^{''} \gets T^{'} \cap N(w,H) \setminus \{w\}$ \;
\nl        \lIf {$P^{''} \neq \emptyset$ \bf{and} $T^{''} \neq \emptyset$}{$future^{'} \gets future^{'} + \langle P^{''} , T^{''} \rangle$}
\nl        \bf{Let} $P^{''} \gets P^{'} \cap N(v,\overline{G}) \setminus \{v\}$ \;
\nl        \bf{Let} $T^{''} \gets T^{'} \cap N(w,\overline{H}) \setminus \{w\}$ \;
\nl        \lIf {$P^{''} \neq \emptyset$ \bf{and} $T^{''} \neq \emptyset$}{$future^{'} \gets future^{'} + \langle P^{''} , T^{''} \rangle$}
       }
\nl   $expand(future^{'},matching)$ \;
\nl   $matching \gets matching \setminus \{(v,w)\}$ \;
  }
\nl $P \gets P \setminus \{v\}$\;
\nl \lIf {$P = \emptyset$} {$future \gets future \setminus \{\langle P,T \rangle \}$}
\nl $expand(future,matching)$ \;
}
\;
\nl $Split(G,H)$ \;
\nl \Begin{
\nl $incumbent \gets \emptyset$ \;
\nl $expand(\{\langle V(G),V(H) \rangle \},\emptyset)$ \;
\nl return $|incubent|$ \;
}
\caption{Lilybank splitting algorithm}
\label{jtAlg}
\end{algorithm}

Now, let's try and explain this. First, assume we have two graph G and H and there are no more vertices in G than in H (think of G as the pattern graph and H as the target).
future is a list of pairs, where the first element of a pair is a set of pattern vertices P and the second is a set of target vertices T. It is assumed that each pattern vertex in P 
might be matched with any (all) of the target vertices in T, or be unmatched. Therefore we start our search via the function LilybankSplit (line 26). This returns an integer, the size of the largest matching.
In line 29 a call is made to expand, where we have as arguments a list of pairs (called future, line 1) and a set of pairs of the form (v,w) where pattern vertex v is matched to target vertex w.

We now focus on expand (line (1)). This is initially called (line 29) with the future as a list containing the single pair $\langle V(G),V(H) \rangle$. Lines 4 to 7 define the conditions 
where we return, i.e. when the incumbent cannot be improved. In line 3, a new incumbent might be saved. Lines 8 to 24 perform the search.

A pair is selected from the future (line 8) using some heuristic. Note that this pair is not removed from the future, and that P and T can be considered as pointers to sets. A pattern vertex is then selected, possibly using a heuristic (line 9). We now iterate over all possible target vertices in T (lines 10 to 21). A target vertex w is selected and a matching produced (line 11). A new future is created (using {\bf Let} to signify a new variable, line 12). Every pair in the future can now be split (lines 13 to 19) into pattern vertices adjacent to the selected pattern vertex v and target vertices adjacent to the selected target vertex w (lines 14 to 15) and this is added to the new future if both sets are not null (line 16) i.e. there is a potential for more matchings. This is then repeated symmetrically for non-adjacency (lines 17 to 19) using the compliments of the graphs.
A recursive call is then made using the new future and the increased matching (line 20). On return from that call we remove the matching (v,w) (line 21). Having explored all matchings with v in its target set we now consider what happens if the selected pattern vertex v is not matched (line 22 to 24), where that set of matchings does not contain a pair (v,*).

\bigskip
\noindent
Phew!

\section{Heuristics}

We ran a small experiment comparing lots of heuristics such as \dots on random
instances. The one where you choose a colour class with as small a max(left size,
right size) as possible seemed to work best. ? Why?

\section{Comparison with Existing Algorithms}

? Describe CP-FC and clique encodings

The Lilybank algorithm explores exactly the same search tree as CP-FC, because \dots

The new algorithm takes $\BigO{n}$ time at each search node. CP-FC takes $\BigO{?}$ time,
and clique takes $\BigO{n^4}$ time.

Our data structure lets us compute good heuristics cheaply, which is why we take
slightly fewer search nodes than CP-FC.

Clique gives us very strong propagation.

Our algorithm needs less space than CP-FC ($\BigO{n^2}$ rather than $\BigO{n^3}$?) and
much less than clique ($\BigO{n^4}$?).

\section{Extensions}

So far, we have considered only unlabelled, undirected graphs. In this section, we
describe some straightforward changes to the algorithm that relax these restrictions.

If we have vertex labels, we replace ${\langle V(G),V(H) \rangle}$ in line 28 of
the algorithm with a set containing a colour class for each label that appears in
both $\V(G)$ and $\V(H)$. In the remainder of this section, \textit{labels} refers to
edge labels.

For the unlabelled, directed case, let $A_G$ and $A_H$ be the adjacency matrices
of $G$ and $H$ respectively, as stored in memory. For each
vertex pair $(t,u)$ in graph $G$ or $H$, the adjacency matrix entry 
takes the value 0 if the vertices are
not adjacent, 1 if the two vertices share a single edge in the direction $t
\rightarrow u$, 2 if they share a single edge in the direction $u \rightarrow
t$, and 3 if there are edges in both directions. Where lines 14-19 of the
basic algorithm split the pair $\langle P',T' \rangle$ two ways, according to whether
vertices are ajacent to $v$ (resp. $w$), we now perform a four-way split where
each vertex is classified according to the label on its adjacency matrix from $v$
(resp. $w$). This is shown in Algorithm~\ref{labDirAlg}, where $L=\{0,1,2,3\}$.

\begin{algorithm}
\DontPrintSemicolon
\nl    \For {$l \in L$}{
\nl        \bf{Let} $P^{''} \gets \{ u \in P^{'} \mid u \neq v \wedge A_G[v][u] = l \}$ \;
\nl        \bf{Let} $T^{''} \gets \{ u \in T^{'} \mid u \neq w \wedge A_H[w][u] = l \}$ \;
\nl        \lIf {$P^{''} \neq \emptyset$ \bf{and} $T^{''} \neq \emptyset$}{$future^{'} \gets future^{'} + \langle P^{''} , T^{''} \rangle$}
       }
\caption{Replacement for lines 14--19 in directed in labelled cases}
\label{labDirAlg}
\end{algorithm}

In the labelled, undirected case, each adjacency-matrix entry contains the edge
label, or a null entry 0 if no edge exists between the pair of vertices.  we
can also use Algorithm~\ref{labDirAlg}, by letting $L$ be the union of $\{0\}$
with the set of all labels that appear in the input graphs. Since there may be
up to $n_G + n_H$ distinct labels, the loop in Algorithm~\ref{labDirAlg} may
execute up to $n_G + n_H$ times, and this variation of the algorithm therefore
requires $\BigO{n^2}$ time per search node.  It is possible to modify the
algorithms to run in $\BigO{n \log n}$ per search node; we will give a brief
outline of the method. First, run lines 17-19 of Algorithm~\ref{jtAlg} to
create a new label-class of vertices that are not adjacent to $v$ or $w$, and
remove these vertices from $\langle P',T' \rangle$. Next, sort $P'$ and $T'$ in
ascending order of the label on the edge from $v$ or $w$ to each vertex. We can
then create the colour classes corresponding to each edge label by
simulataneously traversing $P'$ and $T'$ from left to right.

The labelled, directed case is similar to its undirected counterpart, except that
each element $A[u][v]$ in an adjacency matrix is a pair $(l_1, l_2)$, where
$l_1$ is the label on the edge $u \rightarrow v$ (or 0 if no edge exists) and $l_2$
is the label on the reverse edge.

For the variant in which the common subgraph must be connected, we consider
only undirected graphs. We modify the algorithm by permitting branching only on
a vertex $v$ that has at least one non-zero element in its bitstring label.
(Cite the first paper that did this.) ? Should we say that smarter propagation
might be possible?

\section{Future Work}

Instead of branching on vertices in $\V(G)$, it would be equally valid to branch on
a vertex in $\V(H)$ in lines 9-23 of the algorithm, since our data structure treats the
two graphs symmetrically. More interestingly, we could choose which graph to branch on
at each search node using some heuristic (perhaps choosing based on whether the $P$ or
$T$ set is smaller).

We could create a hybrid of Lilybank and clique (for example, using Lilybank near the
top of the search tree, but switching to a clique encoding when fewer than some threshold
number of vertices remain to be selected). This might give us most of the benefits of
the clique encoding for labelled graphs, while avoiding the high memory cost and colouring
time of encoding the full instance.

\section{Computational Experiments}

Dual Xeon E5-2640 v2 CPUs, 64GBytes RAM. Parallel experiments 32 threads, using
all 16 physical cores with hyper-threading enabled.

\paragraph{No labels, undirected} are in \cref{figure:plain-cumulative}.

\paragraph{Vertex labels but no edge labels, undirected} are in \cref{figure:33v-cumulative}, and are not in other papers so we should omit this unless we see something interesting.

\paragraph{Vertex and edge labels, not directed} are in \cref{figure:33ve-cumulative}, and are not in other papers so we should omit this unless we see something interesting.

\paragraph{Vertex and edge labels, directed} are in \cref{figure:33ved-cumulative}.

\paragraph{No labels, undirected, connected} are in \cref{figure:plain-connected-cumulative}.

\paragraph{Vertex and edge labels, undirected, connected} are in \cref{figure:33ve-connected-cumulative}.

\paragraph{Large subgraph isomorphism instances} are in \cref{figure:sip-cumulative}.

\paragraph{Compared to CP FC} Nodes are in \cref{figure:plain-james-versus-cp-fc-nodes-scatter} and \cref{figure:33ved-james-versus-cp-fc-nodes-scatter}.

\paragraph{Nodes compared to kdown on SIP} are in \cref{figure:sip-james-versus-kdown-nodes-scatter}.

\begin{figure}
    \centering
    \includegraphics*{gen-graph-plain-cumulative.pdf}
    \caption{MCS Plain instances, cumulative runtimes}\label{figure:plain-cumulative}
\end{figure}

\begin{figure}
    \centering
    \includegraphics*{gen-graph-33v-cumulative.pdf}
    \caption{MCS 33\% vertex labelled, no edge labels, undirected instances, cumulative runtimes}\label{figure:33v-cumulative}
\end{figure}

\begin{figure}
    \centering
    \includegraphics*{gen-graph-33ve-cumulative.pdf}
    \caption{MCS 33\% vertex and edge labelled undirected instances, cumulative runtimes}\label{figure:33ve-cumulative}
\end{figure}

\begin{figure}
    \centering
    \includegraphics*{gen-graph-33ved-cumulative.pdf}
    \caption{MCS 33\% vertex and edge labelled directed instances, cumulative runtimes}\label{figure:33ved-cumulative}
\end{figure}

\begin{figure}
    \centering
    \includegraphics*{gen-graph-plain-connected-cumulative.pdf}
    \caption{Unlabelled undirected instances, connected, cumulative runtimes}\label{figure:plain-connected-cumulative}
\end{figure}

\begin{figure}
    \centering
    \includegraphics*{gen-graph-33ve-connected-cumulative.pdf}
    \caption{MCS 33\% vertex and edge labelled undirected instances, connected, cumulative runtimes}\label{figure:33ve-connected-cumulative}
\end{figure}

\begin{figure}
    \centering
    \includegraphics*{gen-graph-sip-cumulative.pdf}
    \caption{SIP instances, cumulative runtimes}\label{figure:sip-cumulative}
\end{figure}

\begin{figure}
    \centering
    \includegraphics*{gen-graph-plain-james-versus-cp-fc-nodes-scatter.pdf}
    \caption{MCS Plain instances, James vs CP-FC, Nodes (TODO?? heatmapify this
    and exclude timeouts)}\label{figure:plain-james-versus-cp-fc-nodes-scatter}
\end{figure}

\begin{figure}
    \centering
    \includegraphics*{gen-graph-33ved-james-versus-cp-fc-nodes-scatter.pdf}
    \caption{MCS 33\% vertex and edge labelled directed instances, James vs
    CP-FC, Nodes (TODO?? heatmapify this and exclude
    timeouts)}\label{figure:33ved-james-versus-cp-fc-nodes-scatter}
\end{figure}

\begin{figure}
    \centering
    \includegraphics*{gen-graph-sip-james-versus-kdown-nodes-scatter.pdf}
    \caption{SIP instances, James vs kdown, Nodes (TODO?? heatmapify this and exclude
    timeouts)}\label{figure:sip-james-versus-kdown-nodes-scatter}
\end{figure}

\section{Conclusion}

\bibliographystyle{named}
\bibliography{paper}

\end{document}

