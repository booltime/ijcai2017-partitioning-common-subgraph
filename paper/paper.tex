\documentclass[letterpaper]{article}
%\usepackage[pass,showframe]{geometry}

\usepackage{ijcai17}
\usepackage{times}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{microtype}
\usepackage{booktabs}

\usepackage{subfigure}

\usepackage[ruled,vlined]{algorithm2e}
\usepackage{MnSymbol,wasysym}
\usepackage{latexsym}
\usepackage{algpseudocode}
\usepackage{amsmath}

\usepackage{tikz}

\usetikzlibrary{graphs}

\newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}
\newcommand{\citep}[1]{\cite{#1}}

\newcommand{\AlgVar}[1]{\mathit{#1}}

\newcommand{\McSplit}{\textproc{McSplit}}

\newcommand{\nmax}{n_{\max}}

\newcommand{\graphG}{\mathcal{G}}
\newcommand{\graphH}{\mathcal{H}}
\newcommand{\setG}{G}
\newcommand{\setH}{H}

% cref style
\crefname{algorithm}{Algorithm}{Algorithms}
\Crefname{algorithm}{Algorithm}{Algorithms}
\crefname{algocf}{Algorithm}{Algorithms}
\Crefname{algocf}{Algorithm}{Algorithms}
\crefname{figure}{Figure}{Figures}
\Crefname{figure}{Figure}{Figures}
\crefname{table}{Table}{Tables}
\Crefname{table}{Table}{Tables}
\crefname{section}{Section}{Sections}
\Crefname{section}{Section}{Sections}

\newcommand{\lineref}[1]{line~\ref{#1}}
\newcommand{\linerangeref}[2]{\count255=\ref{#1}\advance\count255 by 1 \ifnum\count255=\ref{#2}lines~\ref{#1} and~\ref{#2}\else lines~\ref{#1} to~\ref{#2}\fi}
\newcommand{\Lineref}[1]{Line~\ref{#1}}
\newcommand{\Linerangeref}[2]{\count255=\ref{#1}\advance\count255 by 1 \ifnum\count255=\ref{#2}Lines~\ref{#1} and~\ref{#2}\else Lines~\ref{#1} to~\ref{#2}\fi}

% TODO: use operatorname for these
\DeclareMathOperator{\V}{V}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\invN}{\overline{N}}
\DeclareMathOperator{\vtxlabel}{label}

\newcommand{\BigO}[1]{\ensuremath{\operatorname{O}\left(#1\right)}}

\newcommand{\exampleG} {
    \tikz {
        \graph [nodes={draw, circle, minimum width=.5cm, inner sep=1pt}, circular placement, radius=0.8cm,
                clockwise=5] {
                    1,2,3,4,5;
            1--4; 1--5; 2--3; 2--5; 3--5;
        };
    }
}
\newcommand{\exampleH} {
    \tikz {
        \graph [nodes={draw, circle, minimum width=.5cm, inner sep=1pt}, circular placement, radius=0.8cm,
                clockwise=6, phase=60] {
                    a,b,c,d,e,f;
            a--b; a--c; a--e; b--d; b--f; c--d; c--e; c--f; d--f; e--f;
        };
    }
}

\newcommand{\LabelTables}[3] {
  {\footnotesize
    \centering
    \begin{minipage}[t]{.17\linewidth}
        Mapping

        \bigskip\vspace{-4pt}

        #1
    \end{minipage}
    \quad
    \begin{minipage}[t]{0.31\linewidth}
        \centering
        Labelling of $\graphG$\vspace{-4pt}
        \begin{tabular}[t]{cc}
        \toprule
            Vertex & Label\\
        \midrule
            #2
        \bottomrule
        \end{tabular}
    \end{minipage}
    \quad
    \begin{minipage}[t]{0.31\linewidth}
        \centering
        Labelling of $\graphH$\vspace{-4pt}
        \begin{tabular}[t]{cc}
        \toprule
            Vertex & Label\\
        \midrule
            #3
        \bottomrule
        \end{tabular}
        \medskip
    \end{minipage}
  }
}

\title{A Partitioning Algorithm for Maximum Common Subgraph Problems}
\author{The Anonymous Authors of Paper 2431 \\
Academy of Anonymous Academics, Atlantis \\
anonymous@anony.mouse}

% \thanks{This work was supported by a pile of money from a funding agency.}

\begin{document}

\maketitle

\begin{abstract}
    We introduce a new branch and bound algorithm for the maximum common
    subgraph and maximum common connected subgraph problems which is based
    around vertex labelling and partitioning. Our method in some ways resembles
    a traditional constraint programming approach, but uses a novel compact
    domain store and supporting inference algorithms which dramatically reduce
    the memory and computation requirements during search, and allow better
    dual viewpoint ordering heuristics to be calculated cheaply.  Experiments
    show a speedup of more than an order of magnitude over the state of the
    art, and demonstrate that we can operate on much larger graphs without
    running out of memory.
\end{abstract}

\section{Introduction}

To determine the similarity or difference between two graphs, we must first
find what they have in common
\citep{DBLP:journals/prl/Bunke97,DBLP:journals/prl/FernandezV01,KriegeThesis}.
The \emph{maximum common subgraph} family of problems involves finding a large
graph which is isomorphic to subgraphs of two given graphs simultaneously.
Because graphs are widely used to model real-world phenomena, maximum common
subgraph problems have arisen in molecular science (where graphs represent
molecules)
\citep{DBLP:journals/jcamd/RaymondW02a,Ehrlich:2011,DAM2014,Grindley1993707},
and also in other domains including malware detection
\citep{DBLP:journals/compsec/ParkRS13}, source code analysis
\cite{DBLP:journals/tkde/DjokoCH97}, and computer vision
\cite{DBLP:journals/jair/CookH94}.

Maximum common subgraph problems are NP-hard, and remain challenging
computationally. Recent practical progress has been made by using constraint
programming \citep{DBLP:conf/mco/VismaraV08,DBLP:conf/cp/NdiayeS11,DBLP:conf/cp/McCreeshNPS16} and
mathematical programming \citep{DBLP:journals/dam/BahienseMPS12}, by reducing
to the maximum clique problem \citep{LeviG,DBLP:conf/cp/McCreeshNPS16}, and by
adapting subgraph isomorphism algorithms \citep{UpcomingAAAIPaper}. Some
special cases also have practical polynomial time algorithms
\citep{DBLP:conf/mfcs/DroschinskyKM16,DBLP:conf/sofsem/DroschinskyKM17}.

This paper introduces a new branch and bound algorithm which exploits special
properties of these problems to allow a much faster exploration of the search
space, whilst retaining the filtering and bounding benefits of the constraint
programming approach. We describe the algorithm for the basic maximum common
subgraph problem, and discuss how it may be adapted to handle vertex labels,
edge labels, and the requirement that the found subgraph be connected. We then
present an empirical study of the algorithm, demonstrating that it improves the
state of the art by more than an order of magnitude on the unlabelled variant
of the problem, and showing that it can handle much larger instances than
earlier constraint programming or clique approaches due to lower memory usage.

\section{The \McSplit\ Algorithm}

We initially assume that graphs are unlabelled, undirected and without loops
(\cref{sec:extensions} describes how these restrictions may be relaxed).
The vertex and edge sets of a graph $\graphG$ are denoted $\V(\graphG)$ and $\E(\graphG)$.  The
set of vertices adjacent to vertex $v$ in graph $\graphG$ is called the
\emph{neighbourhood} of $v$, denoted $\N(\graphG, v)$. We denote by $\invN(\graphG, v)$ the
\emph{inverse neighbourhood} of $v$, being the set of the vertices not adjacent
to $v$ (excluding $v$ itself). A \emph{subgraph} of a graph $\graphG$ is a graph
consisting of some of the vertices of $\graphG$, and all of the edges between these
vertices. A \emph{common subgraph} of two graphs is a graph which is
(isomorphic to) a subgraph of two graphs simultaneously, and a \emph{maximum}
common subgraph is one with as many vertices as possible. (In this paper, all
subgraphs are induced. The \emph{maximum common partial subgraph} problem
instead asks for a common non-induced subgraph with as many \emph{edges} as
possible \citep{DBLP:conf/cp/NdiayeS11}.)

Let $\graphG$ and $\graphH$ be the two input graphs to our maximum common subgraph problem.
The orders (vertex counts) of these graphs are denoted $g$ and $h$.

With these definitions established, we now present \McSplit. This algorithm
finds a maximum-cardinality mapping $M^* = \{(v_1, w_1), \dots, (v_{m},
w_{m})\}$ with $|M^*| = m$ vertex pairs, where the $v_i$ are distinct vertices
from $\V(\graphG)$ and the $w_i$ are distinct vertices from $\V(\graphH)$, such
that $v_i$ and $v_j$ are adjacent in $\graphG$ if and only if $w_i$ and $w_j$
are adjacent in $\graphH$.  Given such a mapping, the subgraph of $\graphG$
induced by $\{v_i, \dots, v_{m}\}$ and the subgraph of $\graphH$ induced by
$\{w_i, \dots, w_{m}\}$ are a maximum common subgraph.

\paragraph{Walkthrough} Before discussing the algorithm in detail, we illustrate
the main concepts using the graphs $\graphG$ and $\graphH$ in
\cref{fig:alg1}.  These graphs have a maximum common subgraph with four
vertices; one example is the mapping $\{1a, 2f, 3d, 5b\}$ where vertex $1$ is
assigned to vertex $a$, $2$ is assigned to $f$, $3$ to $d$ and $5$ to $b$.

\begin{figure}[t]
\centering
    \exampleG
    \qquad
    \exampleH
\caption{Example graphs $\graphG$ and $\graphH$.}
\label{fig:alg1}
\end{figure}

The algorithm builds up a mapping $M$, starting with the empty mapping
$\emptyset$. Select a vertex in $\V(\graphG)$ as the first vertex to be mapped; in
our example we will arbitrarily choose vertex $1$. Each of the vertices in
$\V(\graphH)$ to which vertex $1$ may be mapped will be tried in turn, and finally the
possibility where vertex $1$ remains unmatched will be tried.

We begin by mapping vertex $1$ to vertex $a$, giving $M=\{1a\}$.  Now
label each unmatched vertex in $\V(\graphG)$ according to whether it is adjacent to vertex $1$, and
label each unmatched vertex in $\V(\graphH)$ according to whether it is adjacent to vertex $a$,
as shown in \cref{fig:alg2}.  Adjacent
vertices have label 1; non-adjacent vertices have label 0.  We can extend $M$
with a mapping $vw$, with $v \in \V(\graphG)$ and $w \in \V(\graphH)$, if and only
if $v$ and $w$ have the same label.  This property, that two vertices may be
mapped together if and only if they share a label, is the algorithm's main
invariant.

\begin{figure}[h]
    \centering\subfiguretopcaptrue
    \subfigure[][After mapping $1$ to $a$] {
      \LabelTables{$\{1a\}$}
                  {$2$ & 0 \\
                   $3$ & 0 \\
                   $4$ & 1 \\
                   $5$ & 1 \\}
                  {$b$ & 1 \\
                   $c$ & 1 \\
                   $d$ & 0 \\
                   $e$ & 1 \\
                   $f$ & 0 \\}
      \label{fig:alg2}
    }

    \subfigure[][After mapping $2$ to $d$] {
      \LabelTables{$\{1a,2d\}$}
                  {$3$ & 01 \\
                   $4$ & 10 \\
                   $5$ & 11 \\}
                  {$b$ & 11 \\
                   $c$ & 11 \\
                   $e$ & 10 \\
                   $f$ & 01 \\}
      \label{fig:alg3}
    }

    \subfigure[][After mapping $3$ to $f$] {
      \LabelTables{$\{1a,2d,3f\}$}
                  {$4$ & 100 \\
                   $5$ & 111 \\}
                  {$b$ & 111 \\
                   $c$ & 111 \\
                   $e$ & 101 \\}
      \label{fig:alg4}
    }

    \caption{Mapping $M$ and vertex labels during search on example graphs $\graphG$ and $\graphH$ from \cref{fig:alg1}.}
    \label{figure:mcsplit-examples}
\end{figure}

\begin{figure}[ht]
\end{figure}

Next, extend the mapping by pairing a vertex in $\graphG$ with a vertex in $\graphH$ of the
same label; we will choose to map vertex $2$ to vertex $d$, giving $M=\{1a,
2d\}$ (\cref{fig:alg3}).  Each unmapped vertex $v \in \V(\graphG)$ is labelled
with a two-character bit string, indicating its adjacency to each of
the two mapped vertices in $\V(\graphG)$ (vertices $1$ and $2$).  For example, vertex
$3$ is labelled $01$, indicating that it is not adjacent to vertex $1$ but is adjacent
to vertex $2$.  Labels are given to unmapped vertices in $\V(\graphH)$ in a similar fashion,
showing adjacency to matched vertices $a$ and $d$.  Our invariant is
maintained: we can extend $M$ by a vertex pairing if and only if the two
vertices have the same label.

The algorithm backtracks when the incumbent (the largest matching found so far) is at least as large
as a calculated bound given $M$ and the current labelling. To demonstrate how
this bound is calculated, we consider the situation one level deeper in the
search tree shown in \cref{fig:alg4}.

Three vertex labels are used: 100,
101, and 111.  The first two of these only appear in one graph, and therefore
there is no way to add a pair of vertices with label 100 or 101 to the mapping.
The final label, 111, appears once in $\graphG$ and twice in $\graphH$, and therefore at
most one pair with this label can be added to $M$.  Thus, the upper bound on
matching size is $|M| + 1 = 4$. The general formula for the upper bound is
\begin{multline*}
    \mathit{bound} = |M| + \sum_{l \in L} \min\big(|\{ v \in \V(\graphG) : \vtxlabel(v)=l\}|, \\[-0.3cm]
        |\{ v \in \V(\graphH) : \vtxlabel(v)=l \}|\big) \text{,}
\end{multline*} where $L$ is the set of labels used in both graphs.

%When we have explored the full
%search space of matchings containing $\{1a, 2d\}$, we try reassigning $2$ to
%$f$.  Since $d$ and $f$ are the only vertices to which $2$ can be matched given
%the decision to match $1$ to $a$, we lastly explore the possibility that $2$ is
%left unmatched, by giving $2$ the label $\bot$ and selecting another vertex in
%$\V(\graphG)$ to assign.

\paragraph{Label classes} We require only $\BigO{g+h}$ space per level of the
search tree to store labelling information.  This is done by storing a
\emph{label class} as a pair $\langle \setG,\setH \rangle$ for each label $l$ that is
used, where $\setG$ is the set of vertices in $\V(\graphG)$ labelled $l$, and $\setH$ is the
set of vertices in $\V(\graphH)$ labelled $l$. Since there are $g + h$ vertices in the two graphs, at most
$g + h$ label classes can exist at once, and there are at most $g + h$ vertices
in the union of all of the $\setG$ and $\setH$ sets. Furthermore, we do not actually
need to store the bits making up a label---we care only that like-labelled
vertices are kept together, and the label itself is not used. Nor do we need to
store any label class which is present only in one graph but not the other (or
which is not present at all).  Together, these facts allow us to store all the
necessary information in two pairs of flat arrays. One pair of arrays is used
to store a permutation of the vertices in $\setG$ having like-labelled vertices
stored consecutively, together with the size and indices of the start of each label in
turn (in some arbitrary order). The second pair stores the permutation of
vertices in $\setH$, together with label start indices \emph{in the same order} as
they are used in the first pair of arrays. This representation has
similarities to data structures used in partition backtracking
\citep{DBLP:conf/wea/Lopez-PresaA09,DBLP:journals/jsc/McKayP14} and the \citet{DBLP:journals/cacm/BronK73}
clique enumeration algorithm.

\begin{algorithm}[t]
\DontPrintSemicolon
\nl $\FuncSty{search}(\AlgVar{future},M)$ \;
\nl \Begin{
%\nl \lIf {$\AlgVar{future} = \emptyset$ \bf{and} $|M| > |\AlgVar{incumbent}|$}
\nl \lIf {$|M| > |\AlgVar{incumbent}|$}{$\AlgVar{incumbent} \gets M$} \label{StoreIncumbent}
%\nl \lIf {$\AlgVar{future} = \emptyset$}{return}
\medskip
\nl $\AlgVar{bound} \gets |M|  + \sum_{\langle \setG,\setH \rangle \in \AlgVar{future}} \min(|\setG|,|\setH|)$ \label{CalcBound} \;
\nl \lIf {$\AlgVar{bound} \leq |\AlgVar{incumbent}|$}{\KwSty{return}} \label{PruneSearch}
\medskip
\nl $\langle \setG,\setH \rangle \gets \FuncSty{SelectLabelClass}(\AlgVar{future})$ \label{SelectClass} \;
\nl $v \gets \FuncSty{SelectVertex}(\setG)$ \label{SelectVertex} \;
\nl \For {$w \in \setH$ \label{WLoop}} {
    \nl    $M \gets M \cup \{(v,w)\}$ \label{GrowM} \;
\nl    \bf{Let} $\AlgVar{future'} \gets \emptyset$ \label{NewFuture} \;
\nl    \For {$\langle \setG',\setH'\rangle \in future$ \label{InnerLoop}}{
\nl        \bf{Let} $\setG'' \gets \setG' \cap \N(\graphG, v) \setminus \{v\}$ \label{NewPWithEdge} \;
\nl        \bf{Let} $\setH'' \gets \setH' \cap \N(\graphH, w) \setminus \{w\}$ \;
\nl        \If {$\setG'' \neq \emptyset$ \bf{and} $\setH'' \neq \emptyset$}{
\nl            $\AlgVar{future'} \gets \AlgVar{future'} \cup \{\langle \setG'' , \setH'' \rangle\}$ \label{AddToFutureWithEdge}}
\nl        \bf{Let} $\setG'' \gets \setG' \cap \invN(\graphG, v) \setminus \{v\}$ \label{NewPWithoutEdge}  \;
\nl        \bf{Let} $\setH'' \gets \setH' \cap \invN(\graphH, w) \setminus \{w\}$ \;
\nl        \If {$\setG'' \neq \emptyset$ \bf{and} $\setH'' \neq \emptyset$}{
\nl            $\AlgVar{future'} \gets \AlgVar{future'} \cup \{\langle \setG'' , \setH'' \rangle\}$} \label{InnerLoopEnd}
       }
\nl   $\FuncSty{search}(\AlgVar{future'},M)$ \label{ExpandWithV} \;
\nl   $M \gets M \setminus \{(v,w)\}$ \label{ShrinkM} \;
  }
\nl $\setG' \gets \setG \setminus \{v\}$ \label{RemoveV} \;
\nl $\AlgVar{future} \gets \AlgVar{future} \setminus \{\langle \setG,\setH \rangle\}$\;
\nl \lIf {$\setG' \neq \emptyset$} {$\AlgVar{future} \gets \AlgVar{future} \cup \{\langle \setG',\setH \rangle \}$}
\nl $\FuncSty{search}(\AlgVar{future},M)$ \label{ExpandWithoutV} \;
}
\;
\nl $\FuncSty{McSplit}(\graphG,\graphH)$ \label{McSplitFun} \;
\nl \Begin{
    \nl $\KwSty{global}~\AlgVar{incumbent} \gets \emptyset$ \;
\nl $\FuncSty{search}(\{\langle V(\graphG),V(\graphH) \rangle \},\emptyset)$ \label{FirstExpandCall} \;
\nl $\KwSty{return}$~$\AlgVar{incumbent}$ \;
}
\caption{Finding a maximum common subgraph.}
\label{McSplitAlg}
\end{algorithm}

\paragraph{\cref{McSplitAlg} in detail} The recursive procedure,
$\FuncSty{search}$, has two parameters.  The parameter $\AlgVar{future}$ is a
list of label classes, each represented as a $\langle \setG, \setH \rangle$ pair as
described above.  The parameter $M$ is the current mapping of vertices.  On
each call to $\FuncSty{search}$, the invariant holds that a $(v,w)$ pair may be
added to $M$ if and only if $v$ and $w$ belong to the same label class in
$\AlgVar{future}$.

\Lineref{StoreIncumbent} stores the current mapping $M$ if it is large enough
to unseat the incumbent.  \Linerangeref{CalcBound}{PruneSearch} prune the
search when a calculated upper bound is not larger than the incumbent.

The remainder of $\FuncSty{search}$ performs the search.  A label class
$\langle \setG, \setH \rangle$ is selected from $\AlgVar{future}$, where $\setG \subseteq
V(\graphG)$ and $\setH \subseteq V(\graphH)$, using some heuristic (\lineref{SelectClass}).
From this label class, a vertex $v$ is selected from $\setG$ using a heuristic
(\lineref{SelectVertex}). We now iterate over all possible vertices in $\setH$
(\linerangeref{WLoop}{ShrinkM}). A vertex $w \in \setH$ is selected and mapping $M$
is extended (\lineref{GrowM}). A new set of label-classes, $\AlgVar{future'}$,
is created (\lineref{NewFuture}).  Every label-class in $\AlgVar{future}$ can
now be split (\linerangeref{InnerLoop}{InnerLoopEnd}) into two new classes. The
first of these classes (\linerangeref{NewPWithEdge}{AddToFutureWithEdge})
contains vertices in $\setG$ adjacent to $v$ and vertices in $\setH$ adjacent to $w$.
This is added to $\AlgVar{future'}$ if both sets contain at least one vertex.
This is then repeated symmetrically for non-adjacency
(\linerangeref{NewPWithoutEdge}{InnerLoopEnd}). A recursive call is then made
(\lineref{ExpandWithV}), on return from which we remove the mapping $(v,w)$.
Having explored all possible mappings of $v$ with vertices in $\setH$ we now
consider what happens if $v$ is not matched
(\linerangeref{RemoveV}{ExpandWithoutV}).

We start our search at the function $\FuncSty{McSplit}$ (\lineref{McSplitFun}),
with graphs $\graphG$ and $\graphH$ as inputs.  This function returns a mapping of
maximum cardinality.  In \lineref{FirstExpandCall} the initial call is made to
$\FuncSty{search}$; at this point we have a single label-class containing all
vertices, and the mapping $M$ is empty.

\subsection{Heuristics}

Small scale experiments (not presented here) were performed to identify
suitable heuristics for selecting a label class abd a vertex within a label
class selection. Our heuristic of choice selects the label class with the
smallest $\min(|\setG|,|\setH|)$. From this class, a vertex is selected from
$\setG$ with maximum degree if $\graphH$ is sparse and with minimum degree if
$\graphH$ is dense. We discuss this further in \cref{sec:comparison}.

\subsection{Extensions}\label{sec:extensions}

\begin{algorithm}[t]
\DontPrintSemicolon
\nl    \For {$l \in L$}{
\nl        \bf{Let} $\setG'' \gets \{ u \in \setG' : u \neq v \wedge A_G[v][u] = l \}$ \;
\nl        \bf{Let} $\setH'' \gets \{ u \in \setH' : u \neq w \wedge A_H[w][u] = l \}$ \;
\nl        \If {$\setG'' \negmedspace\neq \emptyset$ \bf{and} $\setH'' \negmedspace \neq \emptyset$}{
    \nl $\AlgVar{future'} \gets \AlgVar{future'} + \langle \setG''\negmedspace, \setH'' \rangle$}
       }
    \caption{Replacement for \linerangeref{NewPWithEdge}{InnerLoopEnd} of \cref{McSplitAlg} to handle directed and labelled cases.}
\label{labDirAlg}
\end{algorithm}

Maximum common subgraph problems come in many variants. For example, often
vertices have labels (for example, denoting what kind of atom they represent in
a molecule), and vertices may only be mapped to like-labelled vertices. We now
outline how to adapt \cref{McSplitAlg} to handle this and other cases.

\paragraph{Vertex labels and loops} If vertices are labelled, replace ${\langle
V(\graphG),V(\graphH) \rangle}$ in \lineref{FirstExpandCall} with a set of label classes,
one for each label that appears on at least one vertex of both $\graphG$ and $\graphH$.
Loops may be treated as a label modifier: assuming the original vertex labels
are positive integers, we replace the label $l$ with $-l$ on each vertex that
has a loop.

\paragraph{Directed graphs without edge labels} Let $A_G$ and $A_H$ be
adjacency matrices of $\graphG$ and $\graphH$. For each vertex pair $(t,u)$ in graph $\graphG$ or
$\graphH$, the adjacency matrix entry takes the value 0 if the vertices are not
adjacent, 1 if the two vertices share a single edge in the direction $t
\rightarrow u$, 2 if they share a single edge in the direction $u \rightarrow
t$, and 3 if there are edges in both directions. Where
\linerangeref{NewPWithEdge}{InnerLoopEnd} of the basic algorithm split the
label class $\langle \setG',\setH' \rangle$ in two, we now perform a four-way split
where each vertex is classified according to the label on its adjacency matrix
entry from $v$ or $w$.  This is shown in \cref{labDirAlg}, where
$L=\{0,1,2,3\}$.

\paragraph{Undirected with edge labels} Each adjacency matrix entry contains an
edge label, or a null entry $0$ indicating that no edge is present.  We use \cref{labDirAlg}, by
letting $L$ be the union of $\{0\}$ with the set of all labels that appear in
the input graphs. Since there may be up to $g + h$ distinct labels, the loop in
\cref{labDirAlg} may execute up to $g + h$ times, resulting in $\BigO{(g+h)^2}$
time complexity per search node.  To achieve $\BigO{(g+h) \log (g+h)}$ time
complexity per search node, we can modify the algorithms to use sorting rather
than explicitly looping over all label classes, as follows.  First, run lines
17-19 of \cref{McSplitAlg} to create a new label-class of vertices that are
not adjacent to $v$ or $w$, and remove these vertices from $\langle \setG',\setH'
\rangle$.  Next, sort $\setG'$ and $\setH'$ in ascending order of the label on the edge
from $v$ or $w$ to each vertex. We can then create the label classes
corresponding to each edge label by simultaneously traversing $\setG'$ and $\setH'$
from left to right, in a manner that resembles the merging step of merge sort.

\paragraph{Directed with edge labels} This case is similar to its undirected
counterpart, except that each element $A[u][v]$ in an adjacency matrix is a
pair $(l_1, l_2)$, where $l_1$ is the label on the edge $u \rightarrow v$ (or 0
if no edge exists) and $l_2$ is the label on the reverse edge.

\paragraph{Maximum common \emph{connected} subgraph} In chemistry applications,
it is sometimes desirable to require the common subgraph be connected
\citep{Ehrlich:2011}. We consider only undirected graphs. We may modify \McSplit\ by
permitting branching only on a vertex $v$ that has at least one non-zero
element in its bit-string label, following the scheme described by
\citet{DBLP:conf/mco/VismaraV08}.  We can represent this information compactly,
and without increasing time complexity at each search node, by storing an extra
bit with each label class.  This bit takes the value $1$ if and only if the
vertices in the class are adjacent to at least one vertex in $M$.

\section{Experimental Evaluation}

Experiments were performed on machines with dual Intel Xeon E5-2640 v2 CPUs and
64GBytes RAM. Our algorithm was implemented\footnote{URL removed for anonymous
review} in C++ and compiled using g++ 5.3.0. We compare against
the best constraint programming implementations of
\citet{DBLP:conf/cp/NdiayeS11} and \citet{DBLP:conf/cp/McCreeshNPS16} (CP-FC in
the unlabelled cases, and CP-MAC in the labelled cases, using both branching
and filtering for connected subgraphs), the clique encodings of
\citet{DBLP:conf/cp/McCreeshNPS16}, and the $k{\downarrow}$ algorithm of
\citet{UpcomingAAAIPaper} (which only supports unlabelled, undirected,
unconnected instances).

Our first set of experiments use a database of randomly-generated maximum
common subgraph instances
\citep{DBLP:journals/prl/SantoFSV03,DBLP:journals/jgaa/ConteFV07}.  For
unlabelled instances, we selected the first ten instances from each family
whose members have no more than 50 vertices, for a total of 4,100 instances.
For labelled instances, we selected the first ten instances from every family,
for a total of 8,140 instances with up to 100 vertices; like
\citet{DBLP:conf/cp/McCreeshNPS16}, we use the labelling scheme in which the
number of distinct vertex labels and the number of distinct edge labels is
approximately equal to 33 per cent of the number of vertices in each graph.

\begin{figure}[t]
    \centering\subfiguretopcaptrue
    \subfigure[][Unlabelled, undirected instances] {
        \centering
        \includegraphics*{gen-graph-plain-cumulative.pdf}
        \label{figure:plain-cumulative}
    }

    \subfigure[][Vertex and edge labelled, directed instances] {
        \centering
        \includegraphics*{gen-graph-33ved-cumulative.pdf}
        \label{figure:33ved-cumulative}
    }
    \caption{Cumulative numbers of instances solved over time for the maximum
    common subgraph problem.}\label{figure:mcs-cumulative}
\end{figure}

\paragraph{Unlabelled, undirected}
\cref{figure:plain-cumulative} shows a plot of cumulative run times
against number of problem instances solved.  We may compare
the speed of two algorithms using the horizontal distance between their curves.
For example, we could solve 2,000 of the 4,110 unlabelled undirected instances
using the \McSplit\ algorithm if a time limit of 0.5 seconds per instance were
imposed.  Its nearest competitor, CP-FC, would require a time limit of over 24
seconds per instance to solve the same number of instances.  For any given
number of instances, \McSplit\ is comfortably more than an order of magnitude
faster than its nearer competitor.  Moreover, \McSplit\ was the fastest algorithm
on 87\% of the 3,506 instances that
could be solved by at least one of the four algorithm in less than than
1,000 seconds.

\paragraph{Vertex and edge labels, directed} Cumulative run times for this
class of instances are in \cref{figure:33ved-cumulative}. Again, \McSplit\ is over
an order of magnitude faster than the best existing CP algorithm, which is
CP-MAC in this case. Matching the conclusions of
\citet{DBLP:conf/cp/McCreeshNPS16}, we see that the clique encoding outperforms
the other algorithms---including \McSplit---on these labelled instances, except
in the very easy region of instances that can be solved in well under 100 ms.

\paragraph{Unlabelled, undirected, connected} This class of instances are shown
in \cref{figure:plain-connected-cumulative}.  These results are very similar to
the corresponding experiment in \cref{figure:plain-cumulative} in which the
subgraph is not required to be connected: \McSplit\ is the clear winner by more
than an order of magnitude.

\paragraph{Vertex and edge labels, undirected, connected} For the labelled,
connected case, clique slightly outperforms \McSplit\ on harder instances
(\cref{figure:33ve-connected-cumulative}). However, the gap between the two algorithms
is very narrow, and is probably down to quality of implementation; indeed, the
cumulative curve for \McSplit\ briefly rises above the curve for clique at a
runtime just below 100 seconds. Additionally, \McSplit\ is the clear winner for
easier instances, where the clique encoding is relatively expensive to
construct but trivial to solve.

\begin{figure}[t]
    \centering\subfiguretopcaptrue
    \subfigure[][Unlabelled, undirected, connected instances] {
        \centering
        \includegraphics*{gen-graph-plain-connected-cumulative.pdf}
        \label{figure:plain-connected-cumulative}
    }

    \subfigure[][Vertex and edge labelled, undirected, connected instances] {
        \centering
        \includegraphics*{gen-graph-33ve-connected-cumulative.pdf}
        \label{figure:33ve-connected-cumulative}
    }
    \caption{Cumulative numbers of instances solved over time for the maximum
    common connected subgraph problem.} \label{figure:mcs-connected-cumulative}
\end{figure}

\paragraph{Large subgraph isomorphism instances} We also ran the algorithms on
a set of 5,725 larger instances used in recent studies of subgraph
isomorphism~\citep{DBLP:conf/lion/KotthoffMS16} and maximum common
subgraph~\citep{UpcomingAAAIPaper}.  This benchmark set includes real-world
graphs and graphs generated using random models.  Pattern graphs range from 4
vertices to 900 with a median of 80; target graphs range from 10 vertices to
6,671 with a median of 561. Cumulative runtimes on these instances are shown in
\cref{figure:sip-cumulative}.  This is a challenging set of instances, and more
than half of the instances cannot be solved within a timeout of 1,000 seconds by
any solver. Furthermore, the CP-FC algorithm and the clique encoding run out of
memory on many of the instances (these are treated as timeouts, following
\citet{UpcomingAAAIPaper}).

The basic \McSplit\ is beaten by the $k{\downarrow}$ algorithm of
\citet{UpcomingAAAIPaper} on this dataset. However, we can modify the \McSplit\
algorithm to use a top-down strategy similar to that used by $k{\downarrow}$  by
calling the main \FuncSty{McSplit} method once per goal size ($g, g-1, g-2,
\dots$); we backtrack (\lineref{PruneSearch} of \cref{McSplitAlg}) when the
bound is strictly less than the goal size, and terminate when a solution of the
goal size is found. We expect that this could do well because in many cases the
matching covers nearly all of the smaller graph---indeed,
\cref{figure:sip-cumulative} shows that this approach is the strongest on these
instances, and \McSplit{$\downarrow$} is the best algorithm for every choice of timeout.

\begin{figure}[t]
    \centering
    \includegraphics*{gen-graph-sip-cumulative.pdf}
    \caption{Cumulative numbers of instances solved over time for the maximum
    common connected subgraph problem on the large subgraph isomorphism benchmark
    suite.} \label{figure:sip-cumulative}
\end{figure}

\section{Comparison with Existing Algorithms}\label{sec:comparison}

\begin{figure*}[t]
    \centering\subfiguretopcaptrue
    \subfigure[][Unlabelled, undirected instances] {\label{figure:prettyheatmaps1}
    \includegraphics*{gen-graph-plain-james-versus-cp-fc-nodes-scatter.pdf}
    }\subfigure[][Labelled, directed instances] {\label{figure:prettyheatmaps2}
    \includegraphics*{gen-graph-33ved-james-versus-cp-fc-nodes-scatter.pdf}
    }\subfigure[][Subgraph isomorphism instances]{\label{figure:prettyheatmaps3}
    \includegraphics*{gen-graph-sip-james-versus-kdown-nodes-scatter.pdf}
}
    \caption{Relative search space sizes for instances which were solved by both algorithms within the timeout.}
    \label{figure:prettyheatmaps}
\end{figure*}

Our results so far suggest that \McSplit\ has broadly similar performance
trends to the constraint programming, forward-checking (CP-FC) algorithm of
\citet{DBLP:conf/cp/NdiayeS11}, but with much lower constant factors and memory usage. Indeed,
in \cref{figure:prettyheatmaps1,figure:prettyheatmaps2} we plot the number of
recursive calls made by our algorithm versus the number made by
CP-FC, for the unlabelled and labelled, unconnected problem
instances. (Rather than a simple scatter plot, we use darker colours to
indicate a higher density of points around a location.) We see a close
correlation: we typically do slightly less work, and sometimes do more, but
instances with more than one order of magnitude difference in search space size
are rare.

Why is this? We do not see a similar correlation between our search space size
and that of the clique approach. The key observation is that \McSplit\ may be
considered to be a different version of the constraint CP-FC algorithm, using
an unconventional domain store and more efficient filtering algorithms. We now
explore this relationship further.

In the CP-FC algorithm, each vertex $v \in \V(\graphG)$ is represented by a
variable, whose domain corresponds to the set of vertices in $\V(\graphH)$ to
which $v$ may currently be mapped, with an additional special $\bot$ value
representing an unmapped vertex.  Given a label class $\langle \setG,\setH
\rangle$ in \McSplit\, the vertices in $\setG$ correspond to variables in
CP-FC, and the vertices in $\setH$ to domain values. The label-class
representation of domains is possible because throughout the CP-FC algorithm
for maximum common subgraph, the domains of any two variables are either
identical or disjoint (excluding $\bot$, which is slightly more complicated).
To the best of our knowledge, this observation has not been made previously,
and it is not exploited in other implementations.

CP-FC uses a soft all-different constraint to compute a bound, which requires
running a matching algorithm on a supporting compatibility graph.  Careful
thought shows that \McSplit\ computes the same bound, but using a simple
counting loop---this is only possible because of the disjoint nature of the
domains.

%%% This equivalence to CP-FC makes the correctness of \McSplit\ easy to establish:
%%% if we replace the label-class representation of $\mathit{future}$ with
%%% conventional domain stores, and replace \linerangeref{InnerLoop}{InnerLoopEnd}
%%% of \cref{McSplitAlg} with \cref{cpAlg}.  The pruning of domains carried out in
%%% these lines can be easily seen to be equivalent to the corresponding lines in
%%% \cref{McSplitAlg}.
%%% 
%%% 
%%% \begin{algorithm}
%%% \DontPrintSemicolon
%%% \nl    \For {$u \in \N(\graphG, b)$}{
%%% \nl        remove $v$ from $D'_u$ \;
%%% \nl        remove the inverse neighbourhood of $t$ from $D'_u$ \;
%%%        }
%%% \nl    \For {$u \in \invN(\graphG, b)$}{
%%% \nl        remove $v$ from $D'_u$ \;
%%% \nl        remove the neighbourhood of $t$ from $D'_u$ \;
%%%        }
%%% \caption{Replacement for \linerangeref{InnerLoop}{InnerLoopEnd} in CP algorithm ?? Not sure about this bit, maybe we don't need it}
%%% \label{cpAlg}
%%% \end{algorithm}

A further advantage of our encoding is that it gives us efficient access to a
better branching heuristic. The CP-FC algorithm uses smallest domain first,
which in our algorithm corresponds to branching on a label class with smallest
$|\setH|$. We instead branch on the label class with smallest $\min(|\setG|,|\setH|)$.
This is empirically better, and accounts for much of the difference between
the number of recursive calls made; branching on the smallest $|\setG| |\setH|$ gives
very similar results. This can be viewed as exploiting both smallest domain first,
and the dual viewpoint \citep{DBLP:conf/ecai/Geelen92} of smallest domain
first, simultaneously, but we do not have the overheads of having to maintain
and channel between the dual viewpoint that would be required when using a
conventional domain store.

What about our relationship to the $k{\downarrow}$ of
\citet{UpcomingAAAIPaper}?  \Cref{figure:prettyheatmaps3} plots the number of
recursive calls made by $k{\downarrow}$ and \McSplit\ on each of the subgraph
isomorphism instances. Although \McSplit\ is the faster algorithm overall, it
explores more search nodes than $k{\downarrow}$ for most instances (even taking
into account that $k{\downarrow}$ uses a unit propagation loop, and so measures
the search tree slightly differently). This is the classic tradeoff between
speed and cleverness. A hybrid algorithm could be beneficial here: it could use
$k{\downarrow}$ initially, switching to \McSplit\ when the extra filtering is
ineffective, and finally switching to a clique encoding when fewer than some
threshold number of vertices remain to be selected. This might deliver the
benefits of the clique encoding for labelled graphs, while avoiding the high
memory cost and colouring time of encoding the full instance.

\section{Conclusion}

We have introduced the \McSplit\ algorithm for maximum common subgraph
problems.  This algorithm is more than an order of magnitude faster than the
previous state of the art for unlabelled and undirected instances. We have
shown how the algorithm can be extended for graphs with labels on edges, labels
on vertices, loops, directed edges and the requirement that the resultant graph
be connected.

We believe there is more to be discovered about branching heuristics. There is
also the potential to branch on both sides, that is instead of branching on
vertices in $\V(\graphG)$, it would be equally valid to branch on a vertex in
$\V(\graphH)$, since our data structure treats the two graphs symmetrically. More
interestingly, we could choose which graph to branch on at each search node
using some heuristic (perhaps choosing based on whether the $\setG$ or $\setH$ set is
smaller), so long as we can ensure that the search remains complete.

It would be interesting to see whether these techniques are more broadly
applicable---we suspect that some other problems may have a similar branching
structure which would also benefit from a different domain store
representation. Most obviously, we could solve the induced subgraph isomorphism
problem in the same way (and with nearly no changes to the code), and our
connected variant shows that certain side constraints can also be handled.
However, we cannot solve non-induced subgraph isomorphism this way, nor can
we handle certain richer labelling schemes such as those used in temporal
subgraph isomorphism \citep{DBLP:conf/asunam/RedmondC13}.

\bibliographystyle{named}
\bibliography{paper}

\end{document}

