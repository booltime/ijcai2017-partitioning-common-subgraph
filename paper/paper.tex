\documentclass[letterpaper]{article}
%\usepackage[pass,showframe]{geometry}

\usepackage{ijcai17}
\usepackage{times}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{microtype}
\usepackage{booktabs}

\usepackage{subfigure}

\usepackage[ruled,vlined]{algorithm2e}
\usepackage{MnSymbol,wasysym}
\usepackage{latexsym}
\usepackage{algpseudocode}
\usepackage{amsmath}

\usepackage{tikz}

\usetikzlibrary{graphs}

\newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}
\newcommand{\citep}[1]{\cite{#1}}

\newcommand{\AlgVar}[1]{\mathit{#1}}

\newcommand{\McSplit}{\textproc{McSplit}}

\newcommand{\nmax}{n_{\max}}

% cref style
\crefname{algorithm}{Algorithm}{Algorithms}
\Crefname{algorithm}{Algorithm}{Algorithms}
\crefname{algocf}{Algorithm}{Algorithms}
\Crefname{algocf}{Algorithm}{Algorithms}
\crefname{figure}{Figure}{Figures}
\Crefname{figure}{Figure}{Figures}
\crefname{table}{Table}{Tables}
\Crefname{table}{Table}{Tables}
\crefname{section}{Section}{Sections}
\Crefname{section}{Section}{Sections}

\newcommand{\lineref}[1]{line~\ref{#1}}
\newcommand{\linerangeref}[2]{lines~\ref{#1} to~\ref{#2}}
\newcommand{\Lineref}[1]{Line~\ref{#1}}
\newcommand{\Linerangeref}[2]{Lines~\ref{#1} to~\ref{#2}}

% TODO: use operatorname for these
\DeclareMathOperator{\V}{V}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\invN}{\overline{N}}
\DeclareMathOperator{\vtxlabel}{label}

\newcommand{\BigO}[1]{\ensuremath{\operatorname{O}\left(#1\right)}}

\newcommand{\examplexG}[5] {
    \begin{minipage}{.2\textwidth}
    \tikz {
        \graph [nodes={draw, circle, minimum width=.5cm, inner sep=1pt}, circular placement, radius=0.8cm,
                clockwise=5] {
                    1[label=90:#1],2[label=0:#2],3[label=0:#3],4[label=180:#4],5[label=180:#5];
            1--4; 1--5; 2--3; 2--5; 3--5;
        };
    }
    \end{minipage}
}
\newcommand{\examplexH}[6] {
    \begin{minipage}{.2\textwidth}
    \tikz {
        \graph [nodes={draw, circle, minimum width=.5cm, inner sep=1pt}, circular placement, radius=0.8cm,
                clockwise=6, phase=60] {
                    a[label=0:#1],b[label=0:#2],c[label=0:#3],d[label=180:#4],e[label=180:#5],f[label=180:#6];
            a--b; a--c; a--e; b--d; b--f; c--d; c--e; c--f; d--f; e--f;
        };
    }
    \end{minipage}
}

\newcommand{\LabelTables}[4] {
  \centering
  \begin{minipage}[t]{#1\linewidth}
      Mapping

      \bigskip

      #2
  \end{minipage}
  \quad
  \begin{minipage}[t]{0.3\linewidth}
      \centering
      Labelling of $G$
      \begin{tabular}[t]{cc}
      \toprule
          Vertex & Label\\
      \midrule
          #3
      \bottomrule
      \end{tabular}
  \end{minipage}
  \quad
  \begin{minipage}[t]{0.3\linewidth}
      \centering
      Labelling of $H$
      \begin{tabular}[t]{cc}
      \toprule
          Vertex & Label\\
      \midrule
          #4
      \bottomrule
      \end{tabular}
      \medskip
  \end{minipage}
}

\title{A Partitioning Algorithm for Maximum Common Subgraph Problems\thanks{This work
was supported by a pile of money from a funding agency.}}
\author{The Anonymous Authors of Paper 2431 \\
Academy of Anonymous Academics, Some City, Some Country \\
anonymous@anonymous.anonymous}

\begin{document}

\maketitle

\begin{abstract}
    We introduce a new branch and bound algorithm for the maximum common
    subgraph and maximum common connected subgraph problems which is based
    around vertex labelling and partitioning. Our method in some ways resembles
    a traditional constraint programming approach, but uses a novel compact
    domain store and supporting inference algorithms which dramatically reduce
    the memory and computation requirements during search, and allow better
    dual viewpoint ordering heuristics to be calculated cheaply.  Experiments
    show a speedup of more than an order of magnitude over the state of the
    art, and demonstrate that we can operate on much larger graphs without
    running out of memory.
\end{abstract}

\section{Introduction}

To determine the similarity or difference between two graphs, we must first
find what they have in common
\citep{DBLP:journals/prl/Bunke97,DBLP:journals/prl/FernandezV01,KriegeThesis}.
The \emph{maximum common subgraph} family of problems involves finding a large
graph which is isomorphic to subgraphs of two given graphs simultaneously.
Because graphs are widely used to model real-world phenomena, maximum common
subgraph problems have arisen in molecular science (where graphs represent
molecules)
\citep{DBLP:journals/jcamd/RaymondW02a,Ehrlich:2011,DAM2014,Grindley1993707},
computer vision \cite{DBLP:journals/jair/CookH94}, discovery of repeated
patterns in source code \cite{DBLP:journals/tkde/DjokoCH97} and circuits
\cite{DBLP:journals/jair/CookH94}, and recognition of handwritten characters
\cite{SIWEILU1991617}.

Maximum common subgraph problems are NP-hard, and remain challenging
computationally. Recent practical progress has been made by using constraint
programming \citep{DBLP:conf/cp/NdiayeS11,DBLP:conf/cp/McCreeshNPS16} and
mathematical programming \citep{DBLP:journals/dam/BahienseMPS12}, by reducing
to the maximum clique problem \citep{LeviG,DBLP:conf/cp/McCreeshNPS16}, and by
adapting subgraph isomorphism algorithms \citep{UpcomingAAAIPaper}. This paper
introduces a new branch and bound algorithm which exploits special properties
of these problems to allow a much faster exploration of the search space,
whilst retaining the filtering and bounding benefits of the constraint
programming approach. We describe the algorithm for the basic maximum common
subgraph problem, and discuss how it may be adapted to handle vertex labels,
edge labels, and the requirement that the found subgraph be connected. We then
present an empirical study of the algorithm, demonstrating that it improves the
state of the art by more than an order of magnitude on the unlabelled variant
of the problem, and showing that it can handle much larger instances than
earlier constraint programming or clique approaches due to lower memory usage.

\section{The \McSplit\ Algorithm}

We initially assume that graphs are unlabelled, undirected and without loops
(\cref{sec:extensions} describes how these restrictions may be relaxed).
The vertices and edge sets of a graph $G$ are denoted $\V(G)$ and $\E(G)$.  The
set of vertices adjacent to vertex $v$ in graph $G$ is called the
\emph{neighbourhood} of $v$, denoted $\N(G, v)$. We denote by $\invN(G, v)$ the
\emph{inverse neighbourhood} of $v$, being the set of the vertices not adjacent
to $v$ (excluding $v$ itself). A \emph{subgraph} of a graph $G$ is a graph
consisting of some of the vertices of $G$, and all of the edges between these
vertices. A \emph{common subgraph} of two graphs is a graph which is
(isomorphic to) a subgraph of two graphs simultaneously, and a \emph{maximum}
common subgraph is one with as many vertices as possible. (In this paper, all
subgraphs are induced. The \emph{maximum common partial subgraph} problem
instead asks for a common non-induced subgraph with as many \emph{edges} as
possible.)

Let $G$ and $H$ be the two input graphs to our maximum common subgraph problem.
The orders (vertex counts) of these graphs are denoted $g$ and $h$.

With these definitions established, we now present \McSplit, a branch and bound
algorithm which uses a new vertex-labelling scheme to avoid explicitly
maintaining the set of feasible mappings for each vertex in $\V(G)$.

\McSplit\ finds a maximum-cardinality mapping $M^* = \{(v_1, w_1), \dots,
(v_{m}, w_{m})\}$ with $|M^*| = m$ vertex pairs, where the $v_i$ are distinct members of $\V(G)$ and the
$w_i$ are distinct vertices of $\V(H)$, such that $v_i$ and $v_j$ are adjacent
in $G$ if and only if $w_i$ and $w_j$ are adjacent in $H$. 
Given such a mapping, the subgraph of $G$ induced by $\{v_i, \dots, v_{m}\}$
and the subgraph of $H$ induced by $\{w_i, \dots, w_{m}\}$ are a maximum
common subgraph.

\paragraph{Walkthrough} Before discussing the algorithm in detail, we start
with an illustration of the main concepts using the graphs $G$ and $H$ in
\cref{fig:alg1}.  These graphs have a maximum common subgraph with four
vertices; one example is the mapping $\{1a, 2f, 3d, 5b\}$ where vertex $1$ is
assigned to vertex $a$, $2$ is assigned to $f$, $3$ to $d$ and $5$ to $b$.

\begin{figure}[t]
\centering
    \examplexG{}{}{}{}{}
    \examplexH{}{}{}{}{}{}
\caption{Example graphs $G$ and $H$.}
\label{fig:alg1}
\end{figure}

The algorithm builds up a mapping $M$, starting with the empty mapping
$\emptyset$. Select a vertex in $\V(G)$ as the first vertex to be mapped; in
our example we will arbitrarily choose vertex $1$. Each of the vertices in
$\V(H)$ to which vertex $1$ may be mapped will be tried in turn, and finally the
possibility where vertex $1$ remains unmatched will be tried.

We begin by mapping vertex $1$ to vertex $a$, giving $M=\{1a\}$.  Now label
each unmatched vertex according to whether it is adjacent to vertex $1$ (in
$G$) or vertex $a$ (in $H$), as shown in \cref{fig:alg2}.  Adjacent
vertices have label 1; non-adjacent vertices have label 0.  We can extend $M$
with a vertex mapping $vw$, with $v \in \V(G)$ and $w \in \V(H)$, if and only
if $v$ and $w$ have the same label.  This property, that two vertices may be
mapped together if and only if they share a label, is the algorithm's main
invariant.

\begin{figure}[h]
    \centering
    \subfigure[][After mapping $1$ to $a$] {
      \LabelTables{.15}
                  {$\{1a\}$}
                  {$2$ & 0 \\
                   $3$ & 0 \\
                   $4$ & 1 \\
                   $5$ & 1 \\}
                  {$b$ & 1 \\
                   $c$ & 1 \\
                   $d$ & 0 \\
                   $e$ & 1 \\
                   $f$ & 0 \\}
      \label{fig:alg2}
    }

    \subfigure[][After mapping $2$ to $d$] {
      \LabelTables{.15}
                  {$\{1a,2d\}$}
                  {$3$ & 01 \\
                   $4$ & 10 \\
                   $5$ & 11 \\}
                  {$b$ & 11 \\
                   $c$ & 11 \\
                   $e$ & 10 \\
                   $f$ & 01 \\}
      \label{fig:alg3}
    }

    \subfigure[][After mapping $3$ to $f$] {
      \LabelTables{.2}
                  {$\{1a,2d,3f\}$}
                  {$4$ & 100 \\
                   $5$ & 111 \\}
                  {$b$ & 111 \\
                   $c$ & 111 \\
                   $e$ & 101 \\}
      \label{fig:alg4}
    }

    \caption{Mapping $M$ and vertex labels during search on example graphs $G$ and $H$ from \cref{fig:alg1}.}
    \label{figure:mcsplit-examples}
\end{figure}

\begin{figure}[ht]
\end{figure}

Next, extend the mapping by pairing a vertex in $G$ with a vertex in $H$ of the
same label; we will choose to map vertex $2$ to vertex $d$, giving $M=\{1a,
2d\}$ (\cref{fig:alg3}).  Each unmapped vertex $v \in \V(G)$ is labelled
with a two-character bit string, indicating its adjacency to each of
the two mapped vertices in $\V(G)$ (vertices $1$ and $2$).  For example, vertex
$3$ is labelled $01$, indicating that it is not adjacent to vertex $1$ but is adjacent
to vertex $2$.  Labels are given to unmapped vertices in $\V(H)$ in a similar fashion,
showing adjacency to matched vertices $a$ and $d$.  Our invariant is
maintained: we can extend $M$ by a vertex pairing if and only if the two
vertices have the same label.

The algorithm backtracks when the incumbent (the largest matching found so far) is at least as large
as a calculated bound given $M$ and the current labelling. To demonstrate how
this bound is calculated, we consider the situation one level deeper in the
search tree shown in \cref{fig:alg4}.

Three vertex labels are used: 100,
101, and 111.  The first two of these only appear in one graph, and therefore
there is no way to add a pair of vertices with label 100 or 101 to the mapping.
The final label, 111, appears once in $G$ and twice in $H$, and therefore at
most one pair with this label can be added to $M$.  Thus, the upper bound on
matching size is $|M| + 1 = 4$. The general formula for the upper bound is
\begin{multline*}
    \mathit{bound} = |M| + \sum_{l \in L} \min\big(|\{ v \in \V(G) : \vtxlabel(v)=l\}|, \\[-0.3cm]
        |\{ v \in \V(H) : \vtxlabel(v)=l \}|\big) \text{,}
\end{multline*} where $L$ is the set of labels used in both graphs.

%When we have explored the full
%search space of matchings containing $\{1a, 2d\}$, we try reassigning $2$ to
%$f$.  Since $d$ and $f$ are the only vertices to which $2$ can be matched given
%the decision to match $1$ to $a$, we lastly explore the possibility that $2$ is
%left unmatched, by giving $2$ the label $\bot$ and selecting another vertex in
%$\V(G)$ to assign.

\paragraph{Label classes} We require only $\BigO{g+h}$ space per level of the
search tree to store labelling information.  This is done by storing a
\emph{label class} as a pair $\langle P,T \rangle$ for each label $l$ that is
used, where $P$ is the set of vertices in $\V(G)$ labelled $l$, and $T$ is the
set of vertices in $\V(H)$ labelled $l$\footnote{We adopt terminology from
subgraph isomorphism, where one graph is a pattern graph $P$ and the other the
target graph $T$.}. Since there are $g + h$ vertices in the two graphs, at most
$g + h$ label classes can exist at once, and there are at most $g + h$ vertices
in the union of all of the $P$ and $T$ sets. Furthermore, we do not actually
need to store the bits making up a label---we care only that like-labelled
vertices are kept together, and the label itself is not used. Nor do we need to
store any label class which is present only in one graph but not the other (or
which is not present at all).  Together, these facts allow us to store all the
necessary information in two pairs of flat arrays. One pair of arrays is used
to store a permutation of the vertices in $P$ having like-labelled vertices
stored consecutively, together with the indices of the start of each label in
turn (in some arbitrary order). The second pair stores the permutation of
vertices in $T$, together with label start indices \emph{in the same order} as
they are used in the first pair of arrays. ?? This representation has
similarities to data structures used in partition backtracking and bron
kerbosch.

\begin{algorithm}[t]
\DontPrintSemicolon
\nl $\FuncSty{expand}(\AlgVar{future},M)$ \;
\nl \Begin{
%\nl \lIf {$\AlgVar{future} = \emptyset$ \bf{and} $|M| > |\AlgVar{incumbent}|$}
\nl \lIf {$|M| > |\AlgVar{incumbent}|$}{$save(matching)$} \label{StoreIncumbent}
%\nl \lIf {$\AlgVar{future} = \emptyset$}{return}
\nl $bf \gets 0$ \label{SetBestFutureToZero} \;
\nl \lFor {$\langle P,T \rangle \in \AlgVar{future}$}{$\AlgVar{bf} \gets \AlgVar{bf} + \min(|P|,|T|)$}
\nl \lIf {$|M|  + \AlgVar{bf} \leq |\AlgVar{incumbent}|$}{return} \label{PruneSearch}
\medskip
\nl $\langle P,T \rangle \gets \FuncSty{SelectLabelClass}(\AlgVar{future})$ \label{SelectClass} \;
\nl $v \gets \FuncSty{SelectVertex}(P)$ \label{SelectVertex} \;
\nl \For {$w \in T$ \label{WLoop}} {
    \nl    $M \gets M \cup \{(v,w)\}$ \label{GrowM} \;
\nl    \bf{Let} $\AlgVar{future'} \gets \emptyset$ \label{NewFuture} \;
\nl    \For {$\langle P',T'\rangle \in future$ \label{InnerLoop}}{
\nl        \bf{Let} $P'' \gets P' \cap \N(G, v) \setminus \{v\}$ \label{NewPWithEdge} \;
\nl        \bf{Let} $T'' \gets T' \cap \N(H, w) \setminus \{w\}$ \;
\nl        \If {$P'' \neq \emptyset$ \bf{and} $T'' \neq \emptyset$}{
\nl            $\AlgVar{future'} \gets \AlgVar{future'} + \langle P'' , T'' \rangle$ \label{AddToFutureWithEdge}}
\nl        \bf{Let} $P'' \gets P' \cap \invN(G, v) \setminus \{v\}$ \label{NewPWithoutEdge}  \;
\nl        \bf{Let} $T'' \gets T' \cap \invN(H, w) \setminus \{w\}$ \;
\nl        \If {$P'' \neq \emptyset$ \bf{and} $T'' \neq \emptyset$}{
\nl            $\AlgVar{future'} \gets \AlgVar{future'} + \langle P'' , T'' \rangle$} \label{InnerLoopEnd}
       }
\nl   $\FuncSty{expand}(\AlgVar{future'},M)$ \label{ExpandWithV} \;
\nl   $M \gets M \setminus \{(v,w)\}$ \label{ShrinkM} \;
  }
\nl $P' \gets P \setminus \{v\}$ \label{RemoveV} \;
\nl $\AlgVar{future} \gets \AlgVar{future} \setminus \{\langle P,T \rangle\}$\;
\nl \lIf {$P' \neq \emptyset$} {$\AlgVar{future} \gets \AlgVar{future} \cup \{\langle P',T \rangle \}$}
\nl $\FuncSty{expand}(\AlgVar{future},M)$ \label{ExpandWithoutV} \;
}
\;
\nl $\FuncSty{McSplit}(G,H)$ \label{McSplitFun} \;
\nl \Begin{
\nl $\AlgVar{incumbent} \gets \emptyset$ \;
\nl $\FuncSty{expand}(\{\langle V(G),V(H) \rangle \},\emptyset)$ \label{FirstExpandCall} \;
\nl return $|\AlgVar{incubent}|$ \;
}
\caption{Finding a maximum common subgraph.}
\label{McCliqueAlg}
\end{algorithm}

\paragraph{\cref{McCliqueAlg} in detail} The recursive procedure, $\FuncSty{expand}$, has
two parameters.  The parameter $\AlgVar{future}$ is a list of label classes,
each represented as a $\langle P, T \rangle$ pair as described above.  The
parameter $M$ is the current mapping of vertices.  On each call to
$\FuncSty{expand}$, the invariant holds that a $(v,w)$ pair may be added to $M$
if and only if $v$ and $w$ belong to the same label class in $M$ ?? shouldn't that be future?.

\Lineref{StoreIncumbent} stores the current mapping $M$ if it is large enough
to unseat the incumbent.  \Linerangeref{SetBestFutureToZero}{PruneSearch} prune
the search when a calculated upper bound (best future $\AlgVar{bf}$ plus $|M|$) is not larger
than the incumbent ?? rewrite this as one line using $\sum$?.

The remainder of $\FuncSty{expand}$ performs the search.  A label class
$\langle P, T \rangle$ is selected from $\AlgVar{future}$, where $P \subseteq
V(G)$ and $T \subseteq V(H)$, using some heuristic (\lineref{SelectClass}).
From this label class, a vertex $v$ is selected from $P$ using a heuristic
(\lineref{SelectVertex}). We now iterate over all possible vertices in $T$
(\linerangeref{WLoop}{ShrinkM}). A vertex $w \in T$ is selected and mapping $M$
is extended (\lineref{GrowM}). A new set of label-classes, $\AlgVar{future'}$,
is created (\lineref{NewFuture}).  Every label-class in $\AlgVar{future}$ can
now be split (\linerangeref{InnerLoop}{InnerLoopEnd}) into two new classes. The
first of these classes (\linerangeref{NewPWithEdge}{AddToFutureWithEdge})
contains vertices in $P$ adjacent to $v$ and vertices in $T$ adjacent to $w$.
This is added to $\AlgVar{future'}$ if both sets contain at least one vertex.
This is then repeated symmetrically for non-adjacency
(\linerangeref{NewPWithoutEdge}{InnerLoopEnd}). A recursive call is then made
(\lineref{ExpandWithV}), on return from which we remove the mapping $(v,w)$.
Having explored all possible mappings of $v$ with vertices in $T$ we now
consider what happens if $v$ is not matched
(\linerangeref{RemoveV}{ExpandWithoutV}).

We start our search at the function $\FuncSty{McSplit}$ (\lineref{McSplitFun}),
with graphs $G$ and $H$ as inputs.  This function returns the size of the
largest mapping.  In \lineref{FirstExpandCall} the initial call is made to
$\FuncSty{expand}$; at this point we have a single label-class containing all
vertices, and the mapping $M$ is empty.

\subsection{Heuristics}

?? Reword for here, explain later Small scale experiments (not presented here)
were performed to identify a suitable heuristic for label class selection. The
heuristic used selects the label class  with the smallest $\min(|P|,|T|)$ (a
surrogate for expected branching factor). From this class, a vertex is selected
from $P$ with maximum degree if $H$ is sparse and with minimum degree if $H$ is
dense.

\subsection{Extensions}\label{sec:extensions}

?? This needs a two sentence intro saying that people care about stuff and why.

\paragraph{Vertex labels and loops} If vertices are labelled, replace ${\langle
V(G),V(H) \rangle}$ in \lineref{FirstExpandCall} with a set of label classes,
one for each label that appears on at least one vertex of both $G$ and $H$.
Loops may be treated as a label modifier: assuming the original vertex labels
are positive integers, we replace the label $l$ with $-l$ on each vertex that
has a loop.

\paragraph{Directed graphs without edge labels} Let $A_G$ and $A_H$ be
adjacency matrices of $G$ and $H$. For each vertex pair $(t,u)$ in graph $G$ or
$H$, the adjacency matrix entry takes the value 0 if the vertices are not
adjacent, 1 if the two vertices share a single edge in the direction $t
\rightarrow u$, 2 if they share a single edge in the direction $u \rightarrow
t$, and 3 if there are edges in both directions. Where
\linerangeref{NewPWithEdge}{InnerLoopEnd} of the basic algorithm split the
label class $\langle P',T' \rangle$ in two, we now perform a four-way split
where each vertex is classified according to the label on its adjacency matrix
entry from $v$ or $w$.  This is shown in \cref{labDirAlg}, where
$L=\{0,1,2,3\}$.

\begin{algorithm}[t]
\DontPrintSemicolon
\nl    \For {$l \in L$}{
\nl        \bf{Let} $P^{''} \gets \{ u \in P^{'} : u \neq v \wedge A_G[v][u] = l \}$ \;
\nl        \bf{Let} $T^{''} \gets \{ u \in T^{'} : u \neq w \wedge A_H[w][u] = l \}$ \;
\nl        \If {$P^{''} \negmedspace\neq \emptyset$ \bf{and} $T^{''} \negmedspace \neq \emptyset$}{
    \nl $\AlgVar{future^{'}} \gets \AlgVar{future^{'}} + \langle P^{''}\negmedspace, T^{''} \rangle$}
       }
    \caption{Replacement for \linerangeref{NewPWithEdge}{InnerLoopEnd} of \cref{McCliqueAlg} to handle directed and labelled cases.}
\label{labDirAlg}
\end{algorithm}

\paragraph{Undirected with edge labels} Each adjacency matrix entry contains an
edge label, or a null entry (non-adjacent) $0$.  We use \cref{labDirAlg}, by
letting $L$ be the union of $\{0\}$ with the set of all labels that appear in
the input graphs. Since there may be up to $g + h$ distinct labels, the loop in
\cref{labDirAlg} may execute up to $g + h$ times, resulting in $\BigO{(g+h)^2}$
time complexity per search node.  To achieve $\BigO{(g+h) \log (g+h)}$ time
complexity per search node, we can modify the algorithms to use sorting rather
than explicitly looping over all label classes, as follows.  First, run lines
17-19 of \cref{McCliqueAlg} to create a new label-class of vertices that are
not adjacent to $v$ or $w$, and remove these vertices from $\langle P',T'
\rangle$.  Next, sort $P'$ and $T'$ in ascending order of the label on the edge
from $v$ or $w$ to each vertex. We can then create the label classes
corresponding to each edge label by simultaneously traversing $P'$ and $T'$
from left to right.

\paragraph{Directed with edge labels} This case is similar to its undirected
counterpart, except that each element $A[u][v]$ in an adjacency matrix is a
pair $(l_1, l_2)$, where $l_1$ is the label on the edge $u \rightarrow v$ (or 0
if no edge exists) and $l_2$ is the label on the reverse edge.

\paragraph{Maximum Common \emph{Connected} Subgraph} ?? In applications,
something. The common subgraph must be connected. We consider only undirected
graphs.  Modify \McSplit\ by permitting branching only on a vertex $v$ that has
at least one non-zero element in its bit-string label.  We can represent this
information compactly, and without increasing time complexity at each search
node, by storing an extra bit with each label class.  This bit takes the value
$1$ if and only if the vertices in the class are adjacent to at least one
vertex in $M$. ?? Link to branching scheme papers.

\section{Experimental Evaluation}

Experiments were performed on machines with dual Intel Xeon E5-2640 v2 CPUs and
64GBytes RAM. Our algorithm was implemented in C++ and compiled using g++
5.3.0. For comparison, we compare against the best constraint programming
implementations of \citet{DBLP:conf/cp/NdiayeS11} and
\citet{DBLP:conf/cp/McCreeshNPS16} (CP-FC in the unlabelled cases, and CP-MAC
in the labelled cases, using both branching and filtering for connected
subgraphs), the clique encodings of \citet{DBLP:conf/cp/McCreeshNPS16}, and the
$k\downarrow$ algorithm of \citet{UpcomingAAAIPaper} (which only supports
unlabelled, undirected, unconnected instances).

Our first set of experiments use a database of randomly-generated maximum
common subgraph instances
\citep{DBLP:journals/prl/SantoFSV03,DBLP:journals/jgaa/ConteFV07}.  For
unlabelled instances, we selected the first ten instances from each family
whose members have no more than 50 vertices, for a total of 4,100 instances.
For labelled instances, we selected the first ten instances from every family,
for a total of 8,140 instances with up to 100 vertices; like
\citet{DBLP:conf/cp/McCreeshNPS16}, we use the labelling scheme in which the
number of distinct vertex labels and the number of distinct edge labels is
approximately equal to 33 per cent of the number of vertices in each graph.

\begin{figure}[t]
    \centering
    \subfigure[][Unlabelled, undirected instances] {
        \centering
        \includegraphics*{gen-graph-plain-cumulative.pdf}
        \label{figure:plain-cumulative}
    }

    \subfigure[][Vertex and edge labelled, directed instances] {
        \centering
        \includegraphics*{gen-graph-33ved-cumulative.pdf}
        \label{figure:33ved-cumulative}
    }
    \caption{Cumulative numbers of instances solved over time for the maximum
    common subgraph problem.}\label{figure:mcs-cumulative}
\end{figure}

\paragraph{Unlabelled, undirected}
\cref{figure:plain-cumulative} shows a plot of cumulative run times (x-axis)
against number of problem instances solved (y-axis).  We may compare
the speed of two algorithms using the horizontal distance between their curves.
For example, we could solve 2000 of the 4110 unlabelled undirected instances
using the \McSplit\ algorithm if a time limit of 0.5 seconds per instance were
imposed.  Its nearest competitor, CP-FC, would require a time limit of over 24
seconds per instance to solve the same number of instances.  For any given
number of instances, \McSplit\ is comfortably more than an order of magnitude
faster than its nearer competitor.  Moreover, there are few instances in this
set for which \McSplit\ is not the fastest solver; of the 3506 instances that
could be solved by at least one of \McSplit, CP-FC and clique in less than than
1000 seconds, there were only 126 instances on which \McSplit\ was not strictly
faster than the other two solvers.

\paragraph{Vertex and edge labels, directed} Cumulative run times for this
class of instances are in \cref{figure:33ved-cumulative}. Again, \McSplit\ is over
an order of magnitude faster than the best existing CP algorithm, which is
CP-MAC in this case. Matching the conclusions of
\citet{DBLP:conf/cp/McCreeshNPS16}, we see that the clique encoding outperforms
the other algorithms---including \McSplit---on these labelled instances, except
in the very easy region of instances that can be solved in well under 100 ms.

\paragraph{Unlabelled, undirected, connected} This class of instances are shown
in \cref{figure:plain-connected-cumulative}.  These results are very similar to
the corresponding experiment in \cref{figure:plain-cumulative} in which the
subgraph is not required to be connected.

\paragraph{Vertex and edge labels, undirected, connected} For the labelled,
connected case, clique slightly outperforms \McSplit\ on harder instances
(\cref{figure:33ve-connected-cumulative}). However, the gap between the two algorithms
is very narrow, and is probably down to quality of implementation; indeed, the
cumulative curve for \McSplit\ briefly rises above the curve for clique at a
runtime just below 100 seconds. Additionally, \McSplit\ is the clear winner for
easier instances, where the clique encoding is relatively expensive to
construct but trivial to solve.

\begin{figure}[t]
    \centering
    \subfigure[][Unlabelled, undirected, connected instances] {
        \centering
        \includegraphics*{gen-graph-plain-connected-cumulative.pdf}
        \label{figure:plain-connected-cumulative}
    }

    \subfigure[][Vertex and edge labelled, undirected, connected instances] {
        \centering
        \includegraphics*{gen-graph-33ve-connected-cumulative.pdf}
        \label{figure:33ve-connected-cumulative}
    }
    \caption{Cumulative numbers of instances solved over time for the maximum
    common connected subgraph problem.} \label{figure:mcs-connected-cumulative}
\end{figure}

\paragraph{Large subgraph isomorphism instances} We also ran the algorithms on
a set of 5725 larger instances used in recent studies of subgraph
isomorphism~\citep{DBLP:conf/lion/KotthoffMS16} and maximum common
subgraph~\citep{UpcomingAAAIPaper}.  This benchmark set includes real-world
graphs and graphs generated using random models.  Pattern graphs range from 4
vertices to 900 with a median of 80; target graphs range from 10 vertices to
6671 with a median of 561. Cumulative runtimes on these instances are shown in
\cref{figure:sip-cumulative}.  This is a challenging set of instances, and more
than half of the instances cannot be solved within a timeout of 1000 seconds by
any solver. Furthermore, the CP-FC algorithm and the clique encoding run out of
memory on many of the instances (these are treated as timeouts).

The basic \McSplit\ is beaten by the $k\downarrow$ algorithm of
\citet{UpcomingAAAIPaper} on this dataset. However, we can modify the \McSplit\
algorithm to use a top-down strategy similar to that used by $k\downarrow$  by
calling the main \FuncSty{McSplit} method once per goal size ($g, g-1, g-2,
\dots$); we backtrack (\lineref{PruneSearch} of \cref{McCliqueAlg}) when the
bound is strictly less than the goal size, and terminate when a solution of the
goal size is found. We expect that this could do well because in many cases the
matching covers nearly all of the smaller graph---indeed,
\cref{figure:sip-cumulative} shows that this approach is the strongest on these
instances.

\begin{figure}[t]
    \centering
    \includegraphics*{gen-graph-sip-cumulative.pdf}
    \caption{Cumulative numbers of instances solved over time for the maximum
    common connected subgraph problem on the large subgraph isomorphism benchmark
    suite.} \label{figure:sip-cumulative}
\end{figure}

\section{Comparison with Existing Algorithms}\label{sec:comparison}

Our results so far suggest that \McSplit\ has broadly similar performance
trends to the constraint programming, forward-checking (CP-FC) algorithm of
\citet{DBLP:conf/cp/NdiayeS11}, but with much lower constant factors. Indeed,
in \cref{figure:33ved-james-versus-cp-fc-nodes-scatter} we plot the number of
recursive calls made by our algorithm on the $y$-axis versus the number made by
CP-FC on the $x$-axis, for the labelled, unconnected problem instances. We see
a close correlation: we typically do slightly less work, and sometimes do more,
but instances with more than one order of magnitude difference in search space
size are rare.

Why is this? We do not see a similar correlation between our search space size
and that of the clique spproach. The key observation is that \McSplit\ may be
considered to be a different version of the constraint CP-FC algorithm, using
an unconventional domain store and more efficient filtering algorithms. We now
explore this relationship further.

\begin{figure}[t]
    \centering
    \includegraphics*{gen-graph-33ved-james-versus-cp-fc-nodes-scatter.pdf}
    \caption{Size of the search space for \McSplit versus CP-FC on the
    labelled, directed, unconnected instances which were solved within the
    timeout by both algorithms.}
    \label{figure:33ved-james-versus-cp-fc-nodes-scatter}
\end{figure}

In the CP-FC algorithm, each vertex in $v \in \V(G)$ is represented by a
variable, whose domain corresponds to the set of vertices in $\V(H)$ to which
$v$ may currently be mapped, with an additional special $\bot$ value
representing an unmapped vertex.
Given a label class $\langle P,T \rangle$ in \McSplit\, the vertices in $P$
correspond to variables in CP-FC. The label-class representation of domains is
possible because throughout the CP-FC algorithm for max common subgraph, the
domains of any two variables are either identical or disjoint (excluding
$\bot$, which is slightly more complicated). To the best of our knowledge, this
observation has not been made previously, and it is not exploited in other
implementations.

CP-FC uses a soft all-different constraint to compute a bound, which requires
running a matching algorithm on a supporting compatibility graph.  Careful
thought shows that \McSplit\ computes the same bound, but using a simple
counting loop---this is only possible because of the disjoint nature of the
domains.

%%% This equivalence to CP-FC makes the correctness of \McSplit\ easy to establish:
%%% if we replace the label-class representation of $\mathit{future}$ with
%%% conventional domain stores, and replace \linerangeref{InnerLoop}{InnerLoopEnd}
%%% of \cref{McCliqueAlg} with \cref{cpAlg}.  The pruning of domains carried out in
%%% these lines can be easily seen to be equivalent to the corresponding lines in
%%% \cref{McCliqueAlg}.
%%% 
%%% 
%%% \begin{algorithm}
%%% \DontPrintSemicolon
%%% \nl    \For {$u \in \N(G, b)$}{
%%% \nl        remove $v$ from $D'_u$ \;
%%% \nl        remove the inverse neighbourhood of $t$ from $D'_u$ \;
%%%        }
%%% \nl    \For {$u \in \invN(G, b)$}{
%%% \nl        remove $v$ from $D'_u$ \;
%%% \nl        remove the neighbourhood of $t$ from $D'_u$ \;
%%%        }
%%% \caption{Replacement for \linerangeref{InnerLoop}{InnerLoopEnd} in CP algorithm ?? Not sure about this bit, maybe we don't need it}
%%% \label{cpAlg}
%%% \end{algorithm}

A further advantage of our encoding is that it gives us efficient access to a
better branching heuristic. The CP-FC algorithm uses smallest domain first,
which in our algorithm corresponds to branching on a label class with smallest
$|T|$. We instead branch on the label class with smallest $\min(|P|,|T|)$.
This is empirically better, and accounts for much of the difference between
the number of recursive calls made; branching on the smallest $|P| |T|$ gives
very similar results. This can be viewed as exploiting both smallest domain first,
and the dual viewpoint \citep{DBLP:conf/ecai/Geelen92} of smallest domain
first, simultaneously, but we do not have the overheads of having to maintain
and channel between the dual viewpoint that would be required when using a
conventional domain store.

What about our relationship to the $k\downarrow$ of \citet{UpcomingAAAIPaper}?
\Cref{figure:sip-james-versus-kdown-nodes-scatter} plots the number of
recursive calls made by $k\downarrow$ and \McSplit\ on each of the subgraph
isomorphism instances. Although \McSplit\ is the faster algorithm overall, it
explores more search nodes than $k\downarrow$ for most instances.

?? Talk about the classic tradeoff, and dynamically switching.

?? The concentration of points, down to unit propagation and counting

\begin{figure}[h]
    \centering
    \includegraphics*{gen-graph-sip-james-versus-kdown-nodes-scatter.pdf}
    \caption{SIP instances, James vs kdown, Nodes (TODO?? heatmapify this and exclude
    timeouts)}\label{figure:sip-james-versus-kdown-nodes-scatter}
\end{figure}


\section{Conclusion}

We have introduced the \McSplit\ algorithm for Max Common Subgraph.
The new algorithm is more than an order of
magnitude faster than the previous state of the art for unlabelled and undirected
instances. We have shown how the algorithm can be extended for graphs with labels on edges, labels on vertices, loops, directed edges and the requirement that the resultant graph be connected.

\paragraph{Future work} We could create a hybrid algorithm. This would use \McSplit\ near the top of the search tree, switching to a
clique encoding when fewer than some threshold number of vertices remain to be
selected. This might deliver the benefits of the clique encoding for
labelled graphs, while avoiding the high memory cost and colouring time of
encoding the full instance.

We have yet to fully investigate variable and value ordering heuristics, i.e.\ the selection of a label class and the selection of a vertex from that label class.
There is also the potential to branch on both sides, that is instead of branching on vertices in $\V(G)$, it would be equally valid to branch on
a vertex in $\V(H)$ , since our data structure treats the
two graphs symmetrically. More interestingly, we could choose which graph to branch on
at each search node using some heuristic (perhaps choosing based on whether the $P$ or
$T$ set is smaller), so long as we can ensure that the search remains complete.

We have not yet explored parallel versions of the algorithm. The first paralellism is \emph{within-core}, using bitsets to encode sets. The second parallelism  is  multithreaded,
exploited the ubiquitous multicore architecture.

? Portfolio algorithms---finding out at what level of labelledness clique becomes best etc.

%Application to induced subgraph isomorphism (with almost no changes). Applications
%of a similar technique to other problems???

\bibliographystyle{named}
\bibliography{paper}

\end{document}

